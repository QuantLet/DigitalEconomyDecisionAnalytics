{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DEDA Unit 4\n",
    "OOP and Web Scraping Framework\n",
    "Authors: Isabell Fetzer and Junjie Hu\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Web Scraping encompasses any method allowing for extracting data from websites. Requests allows us to send an HTTP request to a webpage. BeautifulSoup parses the HTML in order to retrieve the desired information. \n",
    "Project: We wish to scrape the first page of the South China Morning Post’s news website. We acquire data about the news title, the news link and the news publication date and produce a tabular output stored as .csv file. \n",
    "\"\"\"\n",
    "# Load modules\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from datetime import datetime, date\n",
    "\n",
    "# Receiving source code from the South China Morning Post website\n",
    "scmp_url = 'https://www.scmp.com/knowledge/topics/china-economy/news'\n",
    "url_request = requests.get(scmp_url)\n",
    "\n",
    "# Returns the content of the response\n",
    "url_content = url_request.content\n",
    "\n",
    "# Using BeautifulSoup to parse webpage source code\n",
    "parsed_content = soup(url_content, 'html.parser')\n",
    "\n",
    "# Find all news sections\n",
    "filtered_parts = parsed_content.find_all('div', class_=\"sc-1yocfo6-0\")\n",
    "page_info = []\n",
    "\n",
    "# For loop iterates over every line in text\n",
    "for section in filtered_parts:\n",
    "    unit_info = {}\n",
    "\n",
    "    # (1) Filter title, link, and text content\n",
    "    filtered_part1 = section.find_all('a', class_=\"sc-1ij6sn6-0\")\n",
    "    if len(filtered_part1) < 2:\n",
    "        continue\n",
    "    \n",
    "    # Extract the title and link from the section\n",
    "    news_title = filtered_part1[1].text.strip() if len(filtered_part1) > 1 else ''\n",
    "    news_link = filtered_part1[1].get('href').strip() if len(filtered_part1) > 1 else ''\n",
    "    news_link = f\"https://www.scmp.com{news_link}\"  # adjust the relative link\n",
    "    \n",
    "    # Filter the description text (optional if needed)\n",
    "    news_text = filtered_part1[0].text.strip() if len(filtered_part1) > 0 else ''\n",
    "    \n",
    "    # (2) Filter date\n",
    "    filtered_part2 = section.find_all('time', datetime=True)\n",
    "    if filtered_part2:\n",
    "        try:\n",
    "            # Parse the date format (example format: 2 Aug 2024 - 10:15PM)\n",
    "            news_date = datetime.strptime(filtered_part2[0].text.strip(), '%d %b %Y - %I:%M%p')\n",
    "            news_date = news_date.date()  # only keep the date part\n",
    "        except ValueError:\n",
    "            # If parsing fails, fallback to today's date\n",
    "            news_date = date.today()\n",
    "    else:\n",
    "        news_date = date.today()\n",
    "    \n",
    "    # Add all info into the dictionary\n",
    "    unit_info['news_title'] = news_title\n",
    "    unit_info['news_link'] = news_link\n",
    "    unit_info['news_text'] = news_text\n",
    "    unit_info['news_date'] = news_date\n",
    "    \n",
    "    page_info.append(unit_info)\n",
    "\n",
    "# Print the collected information\n",
    "for info in page_info:\n",
    "    print(info)\n",
    "\n",
    "# Load moduls \n",
    "import pandas as pd\n",
    "import os\n",
    "direct = os.getcwd()\n",
    "# Calling DataFrame constructor on our list \n",
    "df = pd.DataFrame(page_info, columns=['news_title', 'news_link', 'news_time'])\n",
    "print(df)\n",
    "# Exporting to .csv file \n",
    "df.to_csv(direct + '/CSMP_Scraped_News.csv')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Object-Oriented Programming (OOP)\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "OOP aims to structure the program by bundling related properties and characteristics into individual objects. Classes are required to build objects. To define a class we use the keyword 'class' and should start with a capital letter, according to PEP8. Classes defined functions called methods, which identify the behaviours and properties that an object created from the class can perform with its data. \n",
    "\"\"\"\n",
    "# The class Person is inherited from class object\n",
    "class Person(object):\n",
    "    # Using __init__ to initialize a class to take arguments\n",
    "    # self is default argument that points to the instance\n",
    "    def __init__(self, first, last, gender, age):\n",
    "        self.first_name = first\n",
    "        self.last_name = last\n",
    "        self.gender = gender\n",
    "        self.age = age\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "Inheritance allows us to define a class that inherits all the methods and properties from another class. Hereby the class being inherited from is called base class or Parent class and the class that inherits from another class is referred to as derived class or Child class. Python provides the init method which creates instances automatically.\n",
    "\"\"\"\n",
    "class Student(Person):\n",
    "   # The class Student inherited from class Person\n",
    "    def __init__(self, first, last, gender, age, school):\n",
    "     # super() method allows us to handle the arguments from parent class without copying\n",
    "        super().__init__(first, last, gender, age)\n",
    "        # Child class can also be added new arguments\n",
    "        self.school = school\n",
    "    def describe(self):  # describe is a method of class Student\n",
    "        print('{0} {1} is a {2} years old {3} who studies at {4}.'.format( self.first_name,\n",
    "                                                                          self.last_name,\n",
    "                                                                          self.age,\n",
    "                                                                          self.gender,\n",
    "                                                                          self.school))\n",
    "# student1 is an instance of class Student\n",
    "student1 = Student('Laura', 'Doe', 'Female', 10 , 'C_School')\n",
    "print(issubclass(Student, Person))\n",
    "print(isinstance(student1, Student))\n",
    "# Using the attributes in the object student1\n",
    "print(student1.school)\n",
    "# Using the methods in the object student1\n",
    "print(student1.describe())\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "We now alter the code in ReadRSS.py file by using the class. There are some methods with name surrounded by double underscore called ‘magic method’. Further more see: https://www.python-course.eu/python3_magic_methods.php\n",
    "\"\"\"\n",
    "# !pip install feedparser\n",
    "import feedparser\n",
    "class ReadRSS(object):\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.response = feedparser.parse(self.url)\n",
    "        \n",
    "    def __eq__(self, other):\n",
    "        # __eq__() is a magic method that enables comparison two object with ==\n",
    "        if self.url == other.url:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def __repr__(self):\n",
    "    # __repr__() is a magic method that enables customization default printed format\n",
    "        return \"The website url is: \" + self.url\n",
    "\n",
    "    def get_titles(self):\n",
    "        titles = []\n",
    "        for item in self.response[\"entries\"]:\n",
    "            titles.append(item[\"title\"])\n",
    "        print(\"\\nTITLES:\\n\")\n",
    "        print('\\n'.join(titles))\n",
    "        return titles\n",
    "\n",
    "    def get_specificitem(self, item_name):\n",
    "        scripts = []\n",
    "        for item in self.response[\"entries\"]:\n",
    "            scripts.append(item[f\"{item_name}\"])\n",
    "        print(f\"\\n{item_name}:\\n\")\n",
    "        print('\\n'.join(scripts))\n",
    "        return scripts\n",
    "    \n",
    "# ReadRSSClass is the file name of the module code\n",
    "r = ReadRSS(\"https://feeds.a.dj.com/rss/RSSMarketsMain.xml\")\n",
    "r2 = ReadRSS(\"https://feeds.a.dj.com/rss/RSSMarketsMain.xml\")\n",
    "print(r)\n",
    "print(f'The type of r is: {type(r)}')\n",
    "if r == r2: # Here we use == to validate if two responses of two url are equal\n",
    "    print(\"Two urls are the same\")\n",
    "else:\n",
    "    print(\"Two urls are not the same\")\n",
    "# Print out the titles\n",
    "titles = r.get_titles()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Application: On Scraping Daily Weather Report of China Cities\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "This is a preliminary tutorial for scraping web pages\n",
    "With a lot of comments, one can easily get touch web scraping with Python\n",
    "Python Version: 3.6\n",
    "@Author: Junjie Hu, jeremy.junjie.hu@gmail.com\n",
    "\"\"\"\n",
    "# Import all the packages you need, always remember that you can find 99% packages you need in python\n",
    "import requests  # take the website source code back to you\n",
    "import urllib  # some useful functions to deal with website URLs\n",
    "from bs4 import BeautifulSoup as soup  # a package to parse website source code\n",
    "import numpy as np  # all the numerical calculation related methods\n",
    "import re  # regular expression package\n",
    "import itertools  # a package to do iteration works\n",
    "import pickle  # a package to save your file temporarily\n",
    "import pandas as pd  # process structured data\n",
    "\n",
    "save_path = 'output/'  # the path you save your files\n",
    "\n",
    "base_link = 'http://www.tianqihoubao.com/lishi/'  # This link can represent the domain of a series of websites\n",
    "\n",
    "def city_collection():\n",
    "    request_result = requests.get(base_link)  # get source code\n",
    "    parsed = soup(request_result.content)  # parse source code\n",
    "\n",
    "    dt_items = parsed.find_all('dt')  # find the items with tag named 'dt'\n",
    "    for item in dt_items:\n",
    "        # iterate within all the items\n",
    "        province_name = item.text.strip()  # get name of the province\n",
    "        province_link2cities = item.find('a')['href']  # get link to all the cities in the province\n",
    "        province = {'province_link': province_link2cities}\n",
    "        provinces[province_name] = province  # save dict in the dict\n",
    "\n",
    "    for province in provinces.keys():\n",
    "        # iterate with the province link to find all the cities\n",
    "        cities = {}\n",
    "        print(provinces[province]['province_link'])\n",
    "        request_province = requests.get(urllib.parse.urljoin(base_link, provinces[province]['province_link']))\n",
    "        # use the urllib package to join relative links in the proper way\n",
    "        parsed_province = soup(request_province.content)\n",
    "        dd_items = parsed_province.find_all('dd')\n",
    "        for dd_item in dd_items:\n",
    "            print(dd_item)\n",
    "            cities_items = dd_item.find_all('a')\n",
    "            for city_item in cities_items:\n",
    "                city_name = city_item.text.strip()\n",
    "                city_link = city_item.get('href').split('.')[0]\n",
    "                cities[city_name] = city_link\n",
    "        provinces[province]['cities'] = cities\n",
    "    return provinces\n",
    "\n",
    "def weather_collection(link):\n",
    "    \"\"\"\n",
    "    use the link to collect the weather data\n",
    "    :param link: url link\n",
    "    :return: dict, weather of a city everyday\n",
    "    \"\"\"\n",
    "    weather_page_request = requests.get(link)\n",
    "    parsed_page = soup(weather_page_request.content)\n",
    "    tr_items = parsed_page.find_all('tr')\n",
    "    month_weather = dict()\n",
    "    for tr_item in tr_items[1:]:\n",
    "        # print(tr_item)\n",
    "        # daily_weather = dict()\n",
    "        td_items = tr_item.find_all('td')\n",
    "        date = td_items[0].text.strip()\n",
    "        split_pattern = r'[\\n\\r\\s]\\s*'\n",
    "        weather_states = ''.join(re.split(split_pattern, td_items[1].text.strip()))\n",
    "        temperature = ''.join(re.split(split_pattern, td_items[2].text.strip()))\n",
    "        wind = ''.join(re.split(split_pattern, td_items[3].text.strip()))\n",
    "        month_weather[date] = {\n",
    "            'weather': weather_states,\n",
    "            'tempe': temperature,\n",
    "            'wind': wind\n",
    "        }\n",
    "        # month_weather.append(daily_weather)\n",
    "    return month_weather\n",
    "\n",
    "import datetime\n",
    "start_year = 2023\n",
    "end_year = 2024  # This is exclusive, so it will stop at 2020\n",
    "dates = [\n",
    "    (start_year + i // 12, i % 12 + 1)  # Calculate year and month\n",
    "    for i in range((end_year - start_year) * 12)\n",
    "]\n",
    "date = [\n",
    "    f\"{year}{month:02d}\"  # Format the date as 'YYYYMM'\n",
    "    for year, month in dates\n",
    "]\n",
    "\n",
    "#  ==== We have already download the links to all the cities=====\n",
    "#  ==== Otherwise, uncomment the function below to retrieve provinces information ======\n",
    "provinces = dict()  # initialize a dictionary to hold provinces information\n",
    "# This dictionary includes 'province_link' which is the links to find the cities for each province and the 'cities' contains city names and links\n",
    "\n",
    "# provinces_info = city_collection()  # Use this function to retrieve links to all the cities\n",
    "\n",
    "# This is called context management, with open can close the document automatically when the\n",
    "with open('output_cities_link.pkl', 'rb') as cities_file:  # write, change 'rb' -> 'wb'\n",
    "    provinces_info = pickle.load(cities_file)\n",
    "    print(provinces_info)\n",
    "    # pickle.dump(provinces_info, cities_file)  # write\n",
    "\n",
    "weather_record = dict()\n",
    "# The structure is dict in dict\n",
    "# first layer keyword is province name\n",
    "# In each province you can find the cities\n",
    "# In each city, you can find the date, in the date, you can find weather record\n",
    "\n",
    "for key in provinces_info.keys():\n",
    "    # Iterate over different provinces\n",
    "    print(key)\n",
    "    for city_name, city_link in provinces_info[key]['cities'].items():\n",
    "        # Iterate cities within each provinces\n",
    "        print(city_name)\n",
    "        for month_date in date:\n",
    "            # Iterate over different months\n",
    "            print(city_name)\n",
    "            print(month_date)\n",
    "            print(provinces_info[key]['cities'][city_name])\n",
    "            print(\"On Scraping...\")\n",
    "            month_weather = weather_collection(\n",
    "                urllib.parse.urljoin(base_link, city_link) + '/month/' + month_date + '.html')\n",
    "            weather_record[key] = {city_name: {month_date: month_weather}}\n",
    "print('Finished Scraping.')\n",
    "\n",
    "# Exercise: Try to convert the \"json\"-like format to pandas DataFrame"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
